{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "xeSSuzQ8fblr",
        "8pbwfEsAntdJ",
        "GOzmL9dtdWul",
        "ge_qv9PjdGpM",
        "_k_XgrGddMqR",
        "1iz_FwKbOVJ6",
        "HXXgKpKl0uMx",
        "cr2nGUaoRfBs",
        "d3OL0pVD24fH",
        "YvBy476n7bdN",
        "MFkVQTk57-XU",
        "svZK7qHb-DNY",
        "VA08wQRk-KaH",
        "vz3_ha3I-QgI",
        "lwn0u68F-SLL",
        "EN_i1y1M-TqT",
        "FipXC98W-VXy",
        "F2NUliVRK8OF",
        "AlVe6roHWrAS",
        "wyH9Pfe0uXTq",
        "Pt4_FiFOOZwZ",
        "qRiAWXT0Ocnr",
        "_Nt9aJ96tTZC",
        "zwwBZo7Gxz4p",
        "NcQf1l_4TLJb",
        "rAEauTOd_r2q",
        "_dllKITdap0R",
        "0wXi1Wf_j4Pd",
        "d666HO48j_go",
        "NPnFdYCIkB1o",
        "DPrLJ7A_AO25",
        "QVqEOMwhR1Kc",
        "T8AF-laEsD0s",
        "NjkYDoMXsvYZ",
        "WHiSn3aEsr7W",
        "2UKDtDd9t4Jr",
        "5Dv86jPbfpq2",
        "AAb1f6Eksb5_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Path of the data"
      ],
      "metadata": {
        "id": "xeSSuzQ8fblr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "UCj1h9OkfjFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/content/drive/MyDrive/Courses/Data Quality/Project/data.csv\""
      ],
      "metadata": {
        "id": "6AD0oEF6flEb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data importing"
      ],
      "metadata": {
        "id": "8pbwfEsAntdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Packages"
      ],
      "metadata": {
        "id": "GOzmL9dtdWul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sweetviz"
      ],
      "metadata": {
        "id": "g8UbUNk0dVbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Packages"
      ],
      "metadata": {
        "id": "ge_qv9PjdGpM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sweetviz as sv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import scipy.sparse\n",
        "from scipy.stats import zscore\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn import svm\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from scipy.spatial.distance import cosine\n",
        "import matplotlib.gridspec as gridspec"
      ],
      "metadata": {
        "id": "nzw0DWrEdJKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Data"
      ],
      "metadata": {
        "id": "_k_XgrGddMqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start importing the original dataset."
      ],
      "metadata": {
        "id": "IpNeO_3XvErS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9FdPsJOZZmKY"
      },
      "outputs": [],
      "source": [
        "originaldata = pd.read_csv(data_path)\n",
        "originaldata"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We define the function that will create 5 different dataset from the original data: the first with the 50% of missing values, the second with the 40% of missing values, the third with the 30% of missing values, the fourth with the 20% of missing values and the fifth with the 10% of missing values."
      ],
      "metadata": {
        "id": "mazrbPFRvK2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def check_datatypes(df):\n",
        "    for col in df.columns:\n",
        "        if (df[col].dtype == \"bool\"):\n",
        "            df[col] = df[col].astype('string')\n",
        "            df[col] = df[col].astype('object')\n",
        "    return  df\n",
        "\n",
        "def injection(df_pandas, seed, name, name_class): \n",
        "    # the first is the dataset where we want to do the injection\n",
        "    # when we set a seed, it is to control the randomness of the injection. the fu ction with always the same seed will do the same injection\n",
        "    # name class is the class to be predicted, because in the injection this column is the one to be predicted, and so it is the target, used for example in classification\n",
        "    # so we specify this column so that the injection dont touch this column\n",
        "\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    #%%\n",
        "\n",
        "    df_list = []\n",
        "\n",
        "    perc = [0.50, 0.40, 0.30, 0.20, 0.10]\n",
        "    for p in perc:\n",
        "        df_dirt = df_pandas.copy()\n",
        "        comp = [p,1-p]\n",
        "        df_dirt = check_datatypes(df_dirt)\n",
        "\n",
        "        for col in df_dirt.columns:\n",
        "\n",
        "            if col!=name_class:\n",
        "\n",
        "                rand = np.random.choice([True, False], size=df_dirt.shape[0], p=comp)\n",
        "\n",
        "                df_dirt.loc[rand == True,col]=np.nan\n",
        "\n",
        "        df_list.append(df_dirt)\n",
        "        print(\"saved {}-completeness{}%\".format(name, round((1-p)*100)))\n",
        "    return df_list\n",
        "\n",
        "    # so given a dataset, the function return 5 datasets: each one with 10% of missing, 20% of values, 30%, 40% and 50%"
      ],
      "metadata": {
        "id": "cEGq4jssnDPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_no_imputed = injection(df_pandas=originaldata, seed=2, name='data', name_class='income')"
      ],
      "metadata": {
        "id": "5BCDdKX3oCFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = datasets_no_imputed.copy()"
      ],
      "metadata": {
        "id": "mSbt2suz7qUX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data profiling"
      ],
      "metadata": {
        "id": "1iz_FwKbOVJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data profiling is the set of activities and processes designed to determine the metadata of a given dataset. Data profiling helps understand and prepare data for subsequent cleansing, integration, and analysis."
      ],
      "metadata": {
        "id": "c1aTOXrmPWY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic"
      ],
      "metadata": {
        "id": "HXXgKpKl0uMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "originaldata.shape"
      ],
      "metadata": {
        "id": "xBbk-SXXPkMJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols = originaldata.columns\n",
        "cols"
      ],
      "metadata": {
        "id": "1y_0dC8zPo9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "originaldata.dtypes"
      ],
      "metadata": {
        "id": "Pz27Jy5oP43s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_rows = len(originaldata)"
      ],
      "metadata": {
        "id": "QrGcTXcO6rug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cardinalities"
      ],
      "metadata": {
        "id": "cr2nGUaoRfBs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with all the 5 datasets\n",
        "i = 50\n",
        "for dataset in datasets:\n",
        "  print(\"\\n\\n\\nDataset with %d of missing values\\n\" %(i))\n",
        "  for col in dataset.columns:\n",
        "    print('\\n' + col + '\\n')\n",
        "    print(dataset[col].value_counts())\n",
        "  i = i - 10"
      ],
      "metadata": {
        "id": "H2r-3U8URiSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with the original dataset\n",
        "print(\"\\n\\n\\nOriginal dataset\\n\")\n",
        "for col in originaldata.columns:\n",
        "    print('\\n' + col + '\\n')\n",
        "    print(originaldata[col].value_counts())"
      ],
      "metadata": {
        "id": "f8glrgye3lj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Age, fnlwgt, education-num, hours-per-week can be treated as a numeric value\n",
        "* Workclass, education, maritial-status, occupation, relationship, race, sex are categorical, with the values that have not so different frequencies.\n",
        "* Capital-gain and Capital-loss are numeric, but it is good to treat it as a categorical variable, since it seems that they have a particular value that is way more frequent than the others.\n",
        "* Native country is a categorical variable that have a particular value that is way more frequent than the others."
      ],
      "metadata": {
        "id": "X0-v11i3gzSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is important to note that with the replacement of missing values, some categorical values lost some distinct values (eg. Workflow has originally 7 distinct and in the 50% dataset has 6 distinct)"
      ],
      "metadata": {
        "id": "ja7xBb_lZSv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Uniqueness"
      ],
      "metadata": {
        "id": "d3OL0pVD24fH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with original dataset\n",
        "for col in originaldata.columns:\n",
        "  distinct_values = (originaldata[col].nunique())\n",
        "  #count_values = originaldata[col].count()\n",
        "  uniqueness = distinct_values / n_rows\n",
        "  print(\"\\nUniqueness variable\", col,\": \", uniqueness, \"\\n\")"
      ],
      "metadata": {
        "id": "4ge4860a5-DZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the attribute `fnlwgt` seems to be unique for each row, we can remove it."
      ],
      "metadata": {
        "id": "De9iwT7hcTzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "originaldata = originaldata.drop(columns=\"fnlwgt\")"
      ],
      "metadata": {
        "id": "sFqC9Il-cim_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_no_imputed[0] = datasets_no_imputed[0].drop(columns=\"fnlwgt\")\n",
        "datasets_no_imputed[1] = datasets_no_imputed[1].drop(columns=\"fnlwgt\")\n",
        "datasets_no_imputed[2] = datasets_no_imputed[2].drop(columns=\"fnlwgt\")\n",
        "datasets_no_imputed[3] = datasets_no_imputed[3].drop(columns=\"fnlwgt\")\n",
        "datasets_no_imputed[4] = datasets_no_imputed[4].drop(columns=\"fnlwgt\")"
      ],
      "metadata": {
        "id": "WTHn-exysO9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = datasets_no_imputed.copy()"
      ],
      "metadata": {
        "id": "zr0iALAp68Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distinctness"
      ],
      "metadata": {
        "id": "YvBy476n7bdN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# with original dataset\n",
        "for col in originaldata.columns:\n",
        "  distinct_values = (originaldata[col].nunique())\n",
        "  count_values = originaldata[col].count()\n",
        "  distinctness = distinct_values / count_values\n",
        "  print(\"\\nDistinctness variable\", col,\": \",  distinctness, \"\\n\")"
      ],
      "metadata": {
        "id": "mGTHRSzs7e-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pandas Profiling Library"
      ],
      "metadata": {
        "id": "MFkVQTk57-XU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Original data"
      ],
      "metadata": {
        "id": "svZK7qHb-DNY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sweetviz library\n",
        "sweet_report = sv.analyze(originaldata)\n",
        "sweet_report.show_notebook()"
      ],
      "metadata": {
        "id": "YUMYl8D49Tpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset with 50% of missing values"
      ],
      "metadata": {
        "id": "VA08wQRk-KaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sweetviz library\n",
        "sweet_report50 = sv.analyze(datasets[0])\n",
        "sweet_report50.show_notebook()"
      ],
      "metadata": {
        "id": "a690KJM8-Y7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset with 40% of missing values"
      ],
      "metadata": {
        "id": "vz3_ha3I-QgI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sweetviz library\n",
        "sweet_report40 = sv.analyze(datasets[1])\n",
        "sweet_report40.show_notebook()"
      ],
      "metadata": {
        "id": "NAmNtG9G-i4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset with 30% of missing values"
      ],
      "metadata": {
        "id": "lwn0u68F-SLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sweetviz library\n",
        "sweet_report30 = sv.analyze(datasets[2])\n",
        "sweet_report30.show_notebook()"
      ],
      "metadata": {
        "id": "H_-berr_-nnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset with 20% of missing values"
      ],
      "metadata": {
        "id": "EN_i1y1M-TqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sweetviz library\n",
        "sweet_report20 = sv.analyze(datasets[3])\n",
        "sweet_report20.show_notebook()"
      ],
      "metadata": {
        "id": "GaeBQqN9-r_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset with 10% of missing values"
      ],
      "metadata": {
        "id": "FipXC98W-VXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#sweetviz library\n",
        "sweet_report10 = sv.analyze(datasets[4])\n",
        "sweet_report10.show_notebook()"
      ],
      "metadata": {
        "id": "qYbm41Pu-vBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification algorithms"
      ],
      "metadata": {
        "id": "F2NUliVRK8OF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparation"
      ],
      "metadata": {
        "id": "AlVe6roHWrAS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To feed a ML algorithm with data, it should be with a specific structure. The first thing\n",
        "to notice is that the data consists of numerical and categorical data. Since the ML algorithm that I will use will be Logistic regression and SVM, they need the data to be\n",
        "numerical. Thus, I decided to hot-encode the categorical variables. Moreover, I\n",
        "performed a standardization using a z score: all the data will be standardized using the\n",
        "mean and the std dev."
      ],
      "metadata": {
        "id": "ak4-2UBTYLYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preparation(data, categorical_variables, numerical_variables, target_column, splitting_ratio):\n",
        "  \n",
        "  X = data.loc[:, originaldata.columns!=target_column]\n",
        "  t = data[target_column]\n",
        "\n",
        "  categorical_columns = categorical_variables\n",
        "  numerical_columns = numerical_variables\n",
        "\n",
        "  categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\") # processors for categorical columns\n",
        "  numerical_preprocessor = StandardScaler() # processors for numerical columns\n",
        "\n",
        "  preprocessor = ColumnTransformer([\n",
        "      ('one-hot-encoder', categorical_preprocessor, categorical_columns), # trasformer for categorical columns\n",
        "      ('standard_scaler', numerical_preprocessor, numerical_columns)]) # transformer for numerical columns\n",
        "\n",
        "  X = pd.DataFrame.sparse.from_spmatrix(preprocessor.fit_transform(X))   \n",
        "\n",
        "  X = pd.DataFrame(zscore(X.values))\n",
        "  X, t = shuffle(X, t, random_state=0)\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, t, test_size=splitting_ratio, random_state=0)\n",
        "  return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "XpIJ34iBWtt9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performances metrics definitions"
      ],
      "metadata": {
        "id": "wyH9Pfe0uXTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To evaluate the performances of the chosen method, we need to compute the *confusion matrix* which tells us the number of points which have been correctly classified and those which have been misclassified.\n",
        "\n",
        " <table style=\"width:100%\">\n",
        "  <tr>\n",
        "    <th>\n",
        "    <center> <img src='https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg' width=300 /> </center>\n",
        "    </th>\n",
        "    <th>\\begin{array}{|l|l|l|}\n",
        "\t\\hline\n",
        "\t&\\text{Actual Class: 1}\t& \\text{Actual Class: 0}\\\\\n",
        "\t\\hline\n",
        "\t\\text{Predicted Class: 1}\t& tp\t& fp\\\\\n",
        "\t\\hline\n",
        "\t\\text{Predicted Class: 0}\t& fn\t& tn\\\\\n",
        "\t\\hline\n",
        "\\end{array}\n",
        "</th>\n",
        "  </tr>\n",
        "\n",
        "</table> "
      ],
      "metadata": {
        "id": "Zvu7vUsCxCve"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on this matrix we can evaluate:\n",
        "- Accuracy: $Acc = \\frac{tp + tn}{N}$ fraction of the samples correctly classified in the dataset;\n",
        "- Precision $Pre = \\frac{tp}{tp + fp}$ fraction of samples correctly classified in the positive class among the ones classified in the positive class;\n",
        "- Recall: $Rec = \\frac{tp}{tp + fn}$ fraction of samples correctly classified in the positive class among the ones belonging to the positive class;\n",
        "- F1 score: $F1 = \\frac{2 \\cdot Pre \\cdot Rec}{Pre + Rec}$ harmonic mean of the precision and recall;\n",
        "\n",
        "where $tn$ is the number of true negatives, $fp$ is the number of false positives, $fn$ are the false negatives and $tn$ are the true negatives.\n",
        "Equivalently, we can look at the meaning of Precision and Recall by looking at the figure above.\n",
        "\n",
        "Remember that:\n",
        "- The higher these figures of merits the better the algorithm is performing.\n",
        "- These performance measures are **not** symmetric, but depends on the class we selected as positive.\n",
        "- Depending on the **application** one might switch the classes to have measures which better evaluate the predictive power of the classifier."
      ],
      "metadata": {
        "id": "AH_hR2TWxJxM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conf_matrix(y_test, pred_test):    \n",
        "    \n",
        "    # Creating a confusion matrix\n",
        "    con_mat = confusion_matrix(y_test, pred_test)\n",
        "    con_mat = pd.DataFrame(con_mat, range(2), range(2))\n",
        "   \n",
        "    #Ploting the confusion matrix\n",
        "    plt.figure(figsize=(6,6))\n",
        "    sns.set(font_scale=1.5) \n",
        "    sns.heatmap(con_mat, annot=True, annot_kws={\"size\": 16}, fmt='g', cmap='Blues', cbar=False)"
      ],
      "metadata": {
        "id": "tbNlNFKnMqrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression"
      ],
      "metadata": {
        "id": "Pt4_FiFOOZwZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def logistic_regression_classifier(X_tr, X_te, y_tr, y_te):\n",
        "  log_classifier = LogisticRegression(solver='lbfgs', max_iter=1000, class_weight='balanced') # regularization is applied as default\n",
        "  log_classifier.fit(X_tr, y_tr)\n",
        "  y_pred = log_classifier.predict(X_te)\n",
        "\n",
        "  metrics = {\n",
        "      'accuracy': accuracy_score(y_te, y_pred),\n",
        "      'precision_greater': precision_score(y_te, y_pred, pos_label='>50K'),\n",
        "      'precision_smaller': precision_score(y_te, y_pred, pos_label='<=50K'),\n",
        "      'recall_greater': recall_score(y_te, y_pred, pos_label='>50K'),\n",
        "      'recall_smaller': recall_score(y_te, y_pred, pos_label='<=50K'),\n",
        "      'f1_greater': f1_score(y_te, y_pred, pos_label='>50K'),\n",
        "      'f1_smaller': f1_score(y_te, y_pred, pos_label='<=50K'),\n",
        "  }\n",
        "\n",
        "  print(\"\\nMetrics:\\n\")\n",
        "  print(classification_report(y_te, y_pred))\n",
        "  print(\"\\nAccuracy: \", accuracy_score(y_te, y_pred), \"\\n\\n\")\n",
        "  print(\"\\nConfusion Matrix:\\n\")\n",
        "  conf_matrix(y_te, y_pred)\n",
        "\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "de88H-PFSSoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support vector machine"
      ],
      "metadata": {
        "id": "qRiAWXT0Ocnr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SVM_classifier(X_tr, X_te, y_tr, y_te, kernel):\n",
        "  SVM_classifier = svm.SVC(C=1.0, random_state=1, kernel=kernel, class_weight=\"balanced\")\n",
        "  SVM_classifier.fit(X_tr, y_tr)\n",
        "  y_pred = SVM_classifier.predict(X_te)\n",
        "  \n",
        "  metrics = {\n",
        "      'accuracy': accuracy_score(y_te, y_pred),\n",
        "      'precision_greater': precision_score(y_te, y_pred, pos_label='>50K'),\n",
        "      'precision_smaller': precision_score(y_te, y_pred, pos_label='<=50K'),\n",
        "      'recall_greater': recall_score(y_te, y_pred, pos_label='>50K'),\n",
        "      'recall_smaller': recall_score(y_te, y_pred, pos_label='<=50K'),\n",
        "      'f1_greater': f1_score(y_te, y_pred, pos_label='>50K'),\n",
        "      'f1_smaller': f1_score(y_te, y_pred, pos_label='<=50K'),\n",
        "  }\n",
        "\n",
        "  print(\"\\nMetrics:\\n\")\n",
        "  print(classification_report(y_te, y_pred))\n",
        "  print(\"\\nAccuracy: \", accuracy_score(y_te, y_pred), \"\\n\\n\")\n",
        "  print(\"\\nConfusion Matrix:\\n\")\n",
        "  conf_matrix(y_te, y_pred)\n",
        "\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "qFtzHH0ufOGJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML on original data"
      ],
      "metadata": {
        "id": "_Nt9aJ96tTZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression"
      ],
      "metadata": {
        "id": "zwwBZo7Gxz4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = [\"workclass\",\"education\",\"maritial-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\"]\n",
        "numerical_columns = [\"age\",\"education-num\",\"hours-per-week\",\"capital-gain\",\"capital-loss\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = preparation(originaldata, categorical_columns, numerical_columns, \"income\", 0.2)"
      ],
      "metadata": {
        "id": "zFYLcoS2apP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_lr_original = logistic_regression_classifier(X_train, X_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "nPU5vlOId5pt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Support Vector Machine"
      ],
      "metadata": {
        "id": "NcQf1l_4TLJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = [\"workclass\",\"education\",\"maritial-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\"]\n",
        "numerical_columns = [\"age\",\"education-num\",\"hours-per-week\",\"capital-gain\",\"capital-loss\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = preparation(originaldata, categorical_columns, numerical_columns, \"income\", 0.2)"
      ],
      "metadata": {
        "id": "sMda6FmffuEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_svm_original = SVM_classifier(X_train, X_test, y_train, y_test, 'linear')"
      ],
      "metadata": {
        "id": "QHFkpKcBfyX-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Imputation"
      ],
      "metadata": {
        "id": "rAEauTOd_r2q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputation"
      ],
      "metadata": {
        "id": "_dllKITdap0R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the values that each attribute assumes, and at the cardinality of each value, we can choose how to treat each attribute for the imputation:\n",
        "* Age, education-num, hours-per-week, Capital-gain and Capital-loss can be treated as a numeric value, and thus ***ML-based Imputation using KNN*** imputation will be used\n",
        "* Workclass, education, maritial-status, occupation, relationship, race, sex, Native country are categorical. We will use ***Decision Trees*** method\n",
        "\n",
        "The imputation will be done using MICE - Multiple Imputation by Chained Equations.\n",
        "\n",
        "Since there are few reference on MICE with both categorical and numerical variables, a pure custom implementation has been made."
      ],
      "metadata": {
        "id": "xEuIFwUsABQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = datasets_no_imputed.copy()"
      ],
      "metadata": {
        "id": "iXo3K2GxBCfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = [\"workclass\",\"education\",\"maritial-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\"]\n",
        "numerical_columns = [\"age\",\"education-num\",\"hours-per-week\",\"capital-gain\",\"capital-loss\"]\n",
        "number_of_num_columns = len(numerical_columns)"
      ],
      "metadata": {
        "id": "4zAYJ-Q2BDbv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We remove the target from the datasets."
      ],
      "metadata": {
        "id": "h9HR7bdramWc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets[0] = datasets[0].drop(columns=\"income\")\n",
        "datasets[1] = datasets[1].drop(columns=\"income\")\n",
        "datasets[2] = datasets[2].drop(columns=\"income\")\n",
        "datasets[3] = datasets[3].drop(columns=\"income\")\n",
        "datasets[4] = datasets[4].drop(columns=\"income\")"
      ],
      "metadata": {
        "id": "ruTFciuicvsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by imputing only values in the numerical variables."
      ],
      "metadata": {
        "id": "kRQoc4Q9ycCv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_cat = datasets.copy() # the dataset with numerical and categorical variables (13 variables)\n",
        "datasets_aux = list(range(5))\n",
        "i=0\n",
        "for dataset in datasets:\n",
        "  datasets_aux[i] = dataset[numerical_columns].copy()\n",
        "  i=i+1\n",
        "datasets = datasets_aux.copy() # dataset with only numerical"
      ],
      "metadata": {
        "id": "Rgzdvc5X0wse"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
        "\n",
        "multivariate_impute_pipe = ColumnTransformer([\n",
        "    (\"impute_num\", IterativeImputer(estimator=KNeighborsRegressor(n_neighbors=10),\n",
        "                                    max_iter=100), datasets[0].columns)\n",
        "]\n",
        ")\n",
        "datasets_aux = list(range(5))\n",
        "i=0\n",
        "for dataset in datasets:\n",
        "  datasets_aux[i] = pd.DataFrame(multivariate_impute_pipe.fit_transform(dataset), columns=dataset.columns)\n",
        "  i=i+1"
      ],
      "metadata": {
        "id": "n_Mj5QUN1ezr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = datasets_aux.copy() # dataset with only numerical imputed using KNN"
      ],
      "metadata": {
        "id": "-XAXR-enuMYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we consider each categorical column at the time as target of a Decision Tree classifier."
      ],
      "metadata": {
        "id": "Yl9WG2PGJbGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_advanced_imputation = list(range(5)) # we will save here the final datasets imputed\n",
        "i = 0 # we iterate trhough the dataset that have at each iteration a column that will be the target column to predict\n",
        "for dataset in datasets:\n",
        "  col_count = 0 # if we have != 0 columns together with the numerical ones (imputed before), we have to hot encode them\n",
        "\n",
        "  for col in categorical_columns: # we consider a categorical column as target to predict the missings\n",
        "    dataset_to_replace = dataset.copy() # we will use this version of the dataset (with categorical and numerical) and we will add to it the next imputed columns\n",
        "\n",
        "    # check if there are other categorical variables: if yes we hot encode them\n",
        "    if col_count!=0:\n",
        "      no_numerical_variables = number_of_num_columns\n",
        "      cat = dataset.iloc[: , -(originaldata.shape[1]-number_of_num_columns):]\n",
        "\n",
        "      columns_names = list(dataset.columns.values)\n",
        "\n",
        "      cat_columns = columns_names[-(dataset.shape[1]-number_of_num_columns):] # we retrieve the categorical from the dataset, that at each iter will increase in number\n",
        "\n",
        "      categorical_preprocessor = OneHotEncoder(handle_unknown=\"ignore\") # processors for categorical columns\n",
        "\n",
        "      preprocessor = ColumnTransformer([\n",
        "          ('one-hot-encoder', categorical_preprocessor, cat_columns)])\n",
        "      \n",
        "      encoded = pd.DataFrame.sparse.from_spmatrix(preprocessor.fit_transform(dataset))\n",
        "      dataset = pd.concat([dataset.iloc[:,:number_of_num_columns], encoded], axis=1) # we concatenate the numerical variables and the encoded ones\n",
        "\n",
        "    X = dataset.copy()\n",
        "    y = datasets_cat[i][col]\n",
        "\n",
        "    # save indexes of nan and not_nan rows\n",
        "    nans = y.index[y!=y]\n",
        "    no_nans = y.index[y==y]\n",
        "\n",
        "    X = pd.DataFrame(zscore(X.values))\n",
        "\n",
        "    aux = X.assign(target=y)\n",
        "    train = aux.iloc[no_nans]\n",
        "    test = aux.iloc[nans]\n",
        "    X_train = train.iloc[:,:-1]\n",
        "    X_test = test.iloc[:,:-1]\n",
        "    y_train = train.iloc[:,-1]\n",
        "    y_test = test.iloc[:,-1]\n",
        "\n",
        "    clf = DecisionTreeClassifier()\n",
        "    clf = clf.fit(X_train,y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "\n",
        "    final_column = pd.DataFrame(y)\n",
        "    final_column.loc[nans, list(final_column.columns.values)[0]] = y_pred # we take the categorical variables and we replace the nans with the predicted values\n",
        "\n",
        "    predicted_dataset = dataset_to_replace.assign(target=final_column) # we add to the initial dataset with not-hot-encoded categorical variables the predicted and imputed column at this iteration\n",
        "    predicted_dataset.rename(columns = {'target':final_column.columns.values[-1]}, inplace = True)\n",
        "\n",
        "    dataset = predicted_dataset.copy()\n",
        "    col_count=col_count+1\n",
        "\n",
        "  datasets_advanced_imputation[i] = dataset.copy()\n",
        "  i = i + 1\n"
      ],
      "metadata": {
        "id": "jaQu18iHKw_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_advanced_imputation[0] = datasets_advanced_imputation[0].assign(income=originaldata[\"income\"])\n",
        "datasets_advanced_imputation[1] = datasets_advanced_imputation[1].assign(income=originaldata[\"income\"])\n",
        "datasets_advanced_imputation[2] = datasets_advanced_imputation[2].assign(income=originaldata[\"income\"])\n",
        "datasets_advanced_imputation[3] = datasets_advanced_imputation[3].assign(income=originaldata[\"income\"])\n",
        "datasets_advanced_imputation[4] = datasets_advanced_imputation[4].assign(income=originaldata[\"income\"])"
      ],
      "metadata": {
        "id": "djSqayjikjHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We finally change the order of the columns as in the original dataset."
      ],
      "metadata": {
        "id": "YbzdaJGXcoya"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets_advanced_imputation[0] = datasets_advanced_imputation[0].loc[:, list(originaldata.columns.values)]\n",
        "datasets_advanced_imputation[1] = datasets_advanced_imputation[1].loc[:, list(originaldata.columns.values)]\n",
        "datasets_advanced_imputation[2] = datasets_advanced_imputation[2].loc[:, list(originaldata.columns.values)]\n",
        "datasets_advanced_imputation[3] = datasets_advanced_imputation[3].loc[:, list(originaldata.columns.values)]\n",
        "datasets_advanced_imputation[4] = datasets_advanced_imputation[4].loc[:, list(originaldata.columns.values)]"
      ],
      "metadata": {
        "id": "OqZMZbhMcuLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputation accuracy"
      ],
      "metadata": {
        "id": "0wXi1Wf_j4Pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The accuracy for the categorical variables will be the number of the correct imputed values over the number of elements of that variable.\n",
        "* The accuracy for the numerical variables will be calculated as 1 - the cosine similarity between the imputed and the original one."
      ],
      "metadata": {
        "id": "OwmjmYlLcHnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_advanced_imputation = np.empty(5, dtype=object)\n",
        "i = 0\n",
        "for dataset in datasets_advanced_imputation:\n",
        "\n",
        "  # init\n",
        "  variables_accuracies = {}\n",
        "  for col in dataset.columns.values[:-1]:\n",
        "    variables_accuracies[col] = 0\n",
        "\n",
        "  # fullfillement\n",
        "  for col in dataset.columns[:-1]:\n",
        "    if isinstance(dataset[col].iat[0], str):\n",
        "      correct_values = 0\n",
        "      for element1, element2 in zip(dataset[col].values, originaldata[col].values):\n",
        "        if element1 == element2:\n",
        "          correct_values += 1\n",
        "      count_values = dataset[col].count()\n",
        "      accuracy = correct_values/count_values\n",
        "      variables_accuracies[col] = accuracy\n",
        "    else:\n",
        "      variables_accuracies[col] = 1 - cosine(dataset[col], originaldata[col])\n",
        "  accuracy_advanced_imputation[i]=variables_accuracies\n",
        "  i = i + 1"
      ],
      "metadata": {
        "id": "nGlKh4oHcCHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression"
      ],
      "metadata": {
        "id": "d666HO48j_go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = [\"workclass\",\"education\",\"maritial-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\"]\n",
        "numerical_columns = [\"age\",\"education-num\",\"hours-per-week\",\"capital-gain\",\"capital-loss\"]\n",
        "\n",
        "metrics_lr_advanced_imputation = np.empty(5, dtype=object)\n",
        "i = 0\n",
        "for dataset in datasets_advanced_imputation:\n",
        "  X_train, X_test, y_train, y_test = preparation(dataset, categorical_columns, numerical_columns, \"income\", 0.2)\n",
        "  metrics_lr_advanced_imputation[i] = logistic_regression_classifier(X_train, X_test, y_train, y_test)\n",
        "  i = i + 1"
      ],
      "metadata": {
        "id": "95YPOKQdkOGW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "NPnFdYCIkB1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = [\"workclass\",\"education\",\"maritial-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\"]\n",
        "numerical_columns = [\"age\",\"education-num\",\"hours-per-week\",\"capital-gain\",\"capital-loss\"]\n",
        "\n",
        "metrics_svm_advanced_imputation = np.empty(5, dtype=object)\n",
        "i = 0\n",
        "for dataset in datasets_advanced_imputation:\n",
        "  X_train, X_test, y_train, y_test = preparation(dataset, categorical_columns, numerical_columns, \"income\", 0.2)\n",
        "  metrics_svm_advanced_imputation[i] = SVM_classifier(X_train, X_test, y_train, y_test, 'linear')\n",
        "  i = i + 1"
      ],
      "metadata": {
        "id": "QhQHolXvkOit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Standard Imputation"
      ],
      "metadata": {
        "id": "DPrLJ7A_AO25"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the values that each attribute assumes, and at the cardinality of each value, we can choose how to treat each attribute for the imputation:\n",
        "* Age, education-num, hours-per-week can be treated as a numeric value, and thus ***median*** imputation will be used (keeping it without decimal values)\n",
        "* Workclass, education, maritial-status, occupation, relationship, race, sex are categorical. Bfill (or ffill) seems a reasonable choice when we have few missing values, since it is a random imputation. The mode infact could change a lot the dstribution of the data, when the data has lots of missing values. Thus, with 10% and 20% of missing values, the mode imputation seems a good choice, while with 30% and above of missing values a method such that bbfill and ffill that dont change the distribution are reasonable. For simplicity we will use ***bfill*** method\n",
        "* Capital-gain and Capital-loss are numeric, but it is good to treat it as a categorical variable because they can assume few values. Since it seems that they have a particular value that is way more frequent than the others, a ***mode*** imputation seems reasonable for these variables.\n",
        "* Native country is a categorical variable that have a particular value that is way more frequent than the others. A ***mode*** imputation seems reasonable for this variable."
      ],
      "metadata": {
        "id": "xYMv7MibASiQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputation"
      ],
      "metadata": {
        "id": "QVqEOMwhR1Kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = datasets_no_imputed.copy()"
      ],
      "metadata": {
        "id": "WOd2F6ESVbBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we convert the capital-gain and capital-loss column from numeric to categorical\n",
        "for dataset in datasets:\n",
        "  dataset['capital-gain'] = dataset['capital-gain'].astype('category')\n",
        "  dataset['capital-loss'] = dataset['capital-loss'].astype('category')"
      ],
      "metadata": {
        "id": "aFSBxk0_XlmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for dataset in datasets:\n",
        "  for col in dataset.columns:\n",
        "    if col in [\"age\",\"fnlwgt\",\"education-num\",\"hours-per-week\"]:\n",
        "      dataset[col] = dataset[col].fillna(dataset[col].median())\n",
        "    if col in [\"workclass\",\"education\",\"maritial-status\",\"occupation\",\"relationship\",\"race\",\"sex\"]:\n",
        "      dataset[col] = dataset[col].fillna(method=\"bfill\")\n",
        "    if col in [\"capital-gain\",\"capital-loss\",\"native-country\"]:\n",
        "      dataset[col] = dataset[col].fillna(dataset[col].mode()[0])\n",
        "datasets_standard_imputation = datasets.copy()"
      ],
      "metadata": {
        "id": "vHxlo3rLSEYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputation accuracy"
      ],
      "metadata": {
        "id": "T8AF-laEsD0s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The accuracy for the categorical variables will be the number of the correct imputed values over the number of elment of that variable.\n",
        "* The accuracy for the numerical variables will be calculated as 1 - the cosine similarity between the imputed and the original one."
      ],
      "metadata": {
        "id": "YJAYXcpkS8sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_standard_imputation = np.empty(5, dtype=object)\n",
        "i = 0\n",
        "for dataset in datasets_standard_imputation:\n",
        "\n",
        "  # init\n",
        "  variables_accuracies = {}\n",
        "  for col in dataset.columns.values[:-1]:\n",
        "    variables_accuracies[col] = 0\n",
        "\n",
        "  # fullfillement\n",
        "  for col in dataset.columns[:-1]:\n",
        "    if isinstance(dataset[col].iat[0], str):\n",
        "      correct_values = 0\n",
        "      for element1, element2 in zip(dataset[col].values, originaldata[col].values):\n",
        "        if element1 == element2:\n",
        "          correct_values += 1\n",
        "      count_values = dataset[col].count()\n",
        "      accuracy = correct_values/count_values\n",
        "      variables_accuracies[col] = accuracy\n",
        "    else:\n",
        "      variables_accuracies[col] = 1 - cosine(dataset[col], originaldata[col])\n",
        "  accuracy_standard_imputation[i]=variables_accuracies\n",
        "  i = i + 1"
      ],
      "metadata": {
        "id": "UgLBFpEcs11y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic regression"
      ],
      "metadata": {
        "id": "NjkYDoMXsvYZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = [\"workclass\",\"education\",\"maritial-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\"]\n",
        "numerical_columns = [\"age\",\"education-num\",\"hours-per-week\",\"capital-gain\",\"capital-loss\"]\n",
        "\n",
        "metrics_lr_standard_imputation = np.empty(5, dtype=object)\n",
        "i = 0\n",
        "for dataset in datasets_standard_imputation:\n",
        "  X_train, X_test, y_train, y_test = preparation(dataset, categorical_columns, numerical_columns, \"income\", 0.2)\n",
        "  metrics_lr_standard_imputation[i] = logistic_regression_classifier(X_train, X_test, y_train, y_test)\n",
        "  i = i + 1"
      ],
      "metadata": {
        "id": "USX61GiFqKIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "WHiSn3aEsr7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = [\"workclass\",\"education\",\"maritial-status\",\"occupation\",\"relationship\",\"race\",\"sex\",\"native-country\"]\n",
        "numerical_columns = [\"age\",\"education-num\",\"hours-per-week\",\"capital-gain\",\"capital-loss\"]\n",
        "\n",
        "metrics_svm_standard_imputation = np.empty(5, dtype=object)\n",
        "i = 0\n",
        "for dataset in datasets_standard_imputation:\n",
        "  X_train, X_test, y_train, y_test = preparation(dataset, categorical_columns, numerical_columns, \"income\", 0.2)\n",
        "  metrics_svm_standard_imputation[i] = SVM_classifier(X_train, X_test, y_train, y_test, 'linear')\n",
        "  i = i + 1"
      ],
      "metadata": {
        "id": "gM3Jm3y6qKn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion"
      ],
      "metadata": {
        "id": "2UKDtDd9t4Jr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imputed methods accuracy"
      ],
      "metadata": {
        "id": "5Dv86jPbfpq2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, it is possible to note that the imputation on numerical variables is much more\n",
        "precise than the one in the categorical variables. \n",
        "\n",
        "This can be due to the nature of the\n",
        "variable: age, education number and hours per week are variables for which, even with a\n",
        "lot of missing value, it is possible to obtain imputed data that have more or less a similar\n",
        "distribution. For capital gain and capital loss that have a single value as the most frequent,\n",
        "imputation obtains the majority of value close to the most frequent one.\n",
        "\n",
        "It is different for the categorical variables. As I said before, if the data has lots of missing\n",
        "values, it can happen that a specific value of that variable can lose all the observations,\n",
        "with a lack of information that can not be imputed anymore. This problem infact is solved in the numerical one because the predictions can be close to the real value, and also\n",
        "because the accuracy is measured using a similarity like the cosine.\n",
        "\n",
        "The only exception is native country, since it is a variable that has a value that is way the\n",
        "most frequent one, and also because during MICE it was the last categorical variable to be\n",
        "predicted, so it has all the other variables as features.\n"
      ],
      "metadata": {
        "id": "pX1y1_9pYYS4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 18), dpi=80)\n",
        "\n",
        "x = list(originaldata.columns.values[:-1])\n",
        "x_axis = np.arange(len(x))\n",
        "\n",
        "y1 = np.empty(5, dtype=object)\n",
        "i = 0\n",
        "for dataset_measure in accuracy_standard_imputation:\n",
        "  y1[i] = np.empty(len(list(originaldata.columns.values[:-1])), dtype=object)\n",
        "  j = 0\n",
        "  for col in list(originaldata.columns.values[:-1]):\n",
        "    y1[i][j] = dataset_measure[col]\n",
        "    j += 1\n",
        "  i += 1\n",
        "\n",
        "y2 = np.empty(5, dtype=object)\n",
        "i = 0\n",
        "for dataset_measure in accuracy_advanced_imputation:\n",
        "  y2[i] = np.empty(len(list(originaldata.columns.values[:-1])), dtype=object)\n",
        "  j = 0\n",
        "  for col in list(originaldata.columns.values[:-1]):\n",
        "    y2[i][j] = dataset_measure[col]\n",
        "    j += 1\n",
        "  i += 1\n",
        "\n",
        "# Draw first subplot\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.bar(x_axis +0.10, y1[0], width=0.1, label = '50% imputed dataset')\n",
        "plt.bar(x_axis +0.10*2, y1[1], width=0.1, label = '40% imputed dataset')\n",
        "plt.bar(x_axis +0.10*3, y1[2], width=0.1, label = '30% imputed dataset')\n",
        "plt.bar(x_axis +0.10*4, y1[3], width=0.1, label = '20% imputed dataset')\n",
        "plt.bar(x_axis +0.10*5, y1[4], width=0.1, label = '10% imputed dataset')\n",
        "plt.ylim(0.4, 1)\n",
        "\n",
        "plt.xticks(x_axis,x, rotation = 45)\n",
        "plt.title(\"Accuracy of the standard imputated datasets\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Variables\")\n",
        "\n",
        "plt.legend(loc='lower right')\n",
        "# Draw second subplot\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.bar(x_axis +0.10, y2[0], width=0.1, label = '50% imputed dataset')\n",
        "plt.bar(x_axis +0.10*2, y2[1], width=0.1, label = '40% imputed dataset')\n",
        "plt.bar(x_axis +0.10*3, y2[2], width=0.1, label = '30% imputed dataset')\n",
        "plt.bar(x_axis +0.10*4, y2[3], width=0.1, label = '20% imputed dataset')\n",
        "plt.bar(x_axis +0.10*5, y2[4], width=0.1, label = '10% imputed dataset')\n",
        "plt.ylim(0.4, 1)\n",
        "\n",
        "plt.xticks(x_axis,x, rotation = 45)\n",
        "plt.title(\"Accuracy of the advanced imputated datasets\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.xlabel(\"Variables\")\n",
        "\n",
        "plt.legend(loc='lower right')\n",
        "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o_Whbn-OvNJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another factor for the accuracy of the imputation can be the number of distinct values in\n",
        "the categorical variables.\n",
        "\n",
        "The less the distinct values, the closer are the imputed values to the original one. For\n",
        "example, occupation has more distinct values than the other categorical variables and the\n",
        "performance is worse than the others.\n",
        "Regarding the two importation methods, there is an important aspect. In general, the\n",
        "advanced imputation performs better than the standard, especially in education. We can\n",
        "see that in the capital gain and in the capital loss, the accuracy in the 50% missing dataset\n",
        "is lower than the standard one, because in the advanced the variable has been predicted\n",
        "numerically using the KNN, while in the standard we used the mode, and as a result, we\n",
        "kept the imputed values equal to the most frequent one.\n",
        "In general, the trend is that the less missing values are present in the dataset, the better is\n",
        "the similarity between imputed and original variables."
      ],
      "metadata": {
        "id": "j2bHrv7fGAM4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 9), dpi=80)\n",
        "\n",
        "x = list(originaldata.columns.values[:-1])\n",
        "x_axis = np.arange(len(x))\n",
        "\n",
        "y1 = np.empty(5, dtype=object)\n",
        "i = 0\n",
        "for dataset in datasets_no_imputed:\n",
        "  y1[i] = np.empty(len(list(originaldata.columns.values[:-1])), dtype=object)\n",
        "  j = 0\n",
        "  for col in list(originaldata.columns.values[:-1]):\n",
        "    distinct_values = (dataset[col].nunique())\n",
        "    uniqueness = distinct_values / n_rows\n",
        "    y1[i][j] = uniqueness\n",
        "    j += 1\n",
        "  i += 1\n",
        "\n",
        "# Draw first subplot\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.bar(x_axis +0.10, y1[0], width=0.1, label = '50% imputed dataset')\n",
        "plt.bar(x_axis +0.10*2, y1[1], width=0.1, label = '40% imputed dataset')\n",
        "plt.bar(x_axis +0.10*3, y1[2], width=0.1, label = '30% imputed dataset')\n",
        "plt.bar(x_axis +0.10*4, y1[3], width=0.1, label = '20% imputed dataset')\n",
        "plt.bar(x_axis +0.10*5, y1[4], width=0.1, label = '10% imputed dataset')\n",
        "plt.ylim(0, 0.03)\n",
        "\n",
        "plt.xticks(x_axis,x, rotation = 45)\n",
        "plt.title(\"Uniqueness of the standard imputated datasets\")\n",
        "plt.ylabel(\"Uniqueness\")\n",
        "plt.xlabel(\"Variables\")\n",
        "\n",
        "plt.legend(loc='upper center')\n",
        "\n",
        "plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.5)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yMEVRfffGGkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ML algorithms performances"
      ],
      "metadata": {
        "id": "AAb1f6Eksb5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first thing is that logistic regression is performing well and better than SVM. The\n",
        "reason can be multiple, and they depend on the tuning and on the optimization of them.\n",
        "Another aspect is that the logistic regression and the SVM have good performances in the\n",
        "standard imputation even with some missing value. The difference between standard and\n",
        "advanced is that the standard imputation is more unstable than the advanced one, since\n",
        "the latter has a more cleaned trend."
      ],
      "metadata": {
        "id": "Pk36dcYvY5EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(18, 75), dpi=80)\n",
        "\n",
        "AX = gridspec.GridSpec(7,2)\n",
        "AX.update(right=1, top=1, wspace = 0.3, hspace = 0.3)\n",
        "acc_std = plt.subplot(AX[0,0])\n",
        "acc_adv = plt.subplot(AX[0,1])\n",
        "pre1_std = plt.subplot(AX[1,0])\n",
        "pre1_adv = plt.subplot(AX[1,1])\n",
        "pre2_std = plt.subplot(AX[2,0])\n",
        "pre2_adv = plt.subplot(AX[2,1])\n",
        "rec1_std = plt.subplot(AX[3,0])\n",
        "rec1_adv = plt.subplot(AX[3,1])\n",
        "rec2_std = plt.subplot(AX[4,0])\n",
        "rec2_adv = plt.subplot(AX[4,1])\n",
        "f11_std = plt.subplot(AX[5,0])\n",
        "f11_adv = plt.subplot(AX[5,1])\n",
        "f12_std = plt.subplot(AX[6,0])\n",
        "f12_adv = plt.subplot(AX[6,1])\n",
        "\n",
        "  \n",
        "x = ['50%','40%','30%','20%','10%','Original']\n",
        "\n",
        "# accuracy\n",
        "y1_1 = [metrics_lr_standard_imputation[0]['accuracy'],\n",
        "      metrics_lr_standard_imputation[1]['accuracy'],\n",
        "      metrics_lr_standard_imputation[2]['accuracy'],\n",
        "      metrics_lr_standard_imputation[3]['accuracy'],\n",
        "      metrics_lr_standard_imputation[4]['accuracy'],\n",
        "      metrics_lr_original['accuracy']]  \n",
        "y1_2 = [metrics_svm_standard_imputation[0]['accuracy'],\n",
        "      metrics_svm_standard_imputation[1]['accuracy'],\n",
        "      metrics_svm_standard_imputation[2]['accuracy'],\n",
        "      metrics_svm_standard_imputation[3]['accuracy'],\n",
        "      metrics_svm_standard_imputation[4]['accuracy'],\n",
        "      metrics_svm_original['accuracy']]\n",
        "y2_1 = [metrics_lr_advanced_imputation[0]['accuracy'],\n",
        "      metrics_lr_advanced_imputation[1]['accuracy'],\n",
        "      metrics_lr_advanced_imputation[2]['accuracy'],\n",
        "      metrics_lr_advanced_imputation[3]['accuracy'],\n",
        "      metrics_lr_advanced_imputation[4]['accuracy'],\n",
        "      metrics_lr_original['accuracy']]  \n",
        "y2_2 = [metrics_svm_advanced_imputation[0]['accuracy'],\n",
        "      metrics_svm_advanced_imputation[1]['accuracy'],\n",
        "      metrics_svm_advanced_imputation[2]['accuracy'],\n",
        "      metrics_svm_advanced_imputation[3]['accuracy'],\n",
        "      metrics_svm_advanced_imputation[4]['accuracy'],\n",
        "      metrics_svm_original['accuracy']]\n",
        "\n",
        "# precision of income<=50%\n",
        "y3_1 = [metrics_lr_standard_imputation[0]['precision_smaller'],\n",
        "      metrics_lr_standard_imputation[1]['precision_smaller'],\n",
        "      metrics_lr_standard_imputation[2]['precision_smaller'],\n",
        "      metrics_lr_standard_imputation[3]['precision_smaller'],\n",
        "      metrics_lr_standard_imputation[4]['precision_smaller'],\n",
        "      metrics_lr_original['precision_smaller']]  \n",
        "y3_2 = [metrics_svm_standard_imputation[0]['precision_smaller'],\n",
        "      metrics_svm_standard_imputation[1]['precision_smaller'],\n",
        "      metrics_svm_standard_imputation[2]['precision_smaller'],\n",
        "      metrics_svm_standard_imputation[3]['precision_smaller'],\n",
        "      metrics_svm_standard_imputation[4]['precision_smaller'],\n",
        "      metrics_svm_original['precision_smaller']]\n",
        "y4_1 = [metrics_lr_advanced_imputation[0]['precision_smaller'],\n",
        "      metrics_lr_advanced_imputation[1]['precision_smaller'],\n",
        "      metrics_lr_advanced_imputation[2]['precision_smaller'],\n",
        "      metrics_lr_advanced_imputation[3]['precision_smaller'],\n",
        "      metrics_lr_advanced_imputation[4]['precision_smaller'],\n",
        "      metrics_lr_original['precision_smaller']]  \n",
        "y4_2 = [metrics_svm_advanced_imputation[0]['precision_smaller'],\n",
        "      metrics_svm_advanced_imputation[1]['precision_smaller'],\n",
        "      metrics_svm_advanced_imputation[2]['precision_smaller'],\n",
        "      metrics_svm_advanced_imputation[3]['precision_smaller'],\n",
        "      metrics_svm_advanced_imputation[4]['precision_smaller'],\n",
        "      metrics_svm_original['precision_smaller']]\n",
        "\n",
        "# precision of income>50%\n",
        "y5_1 = [metrics_lr_standard_imputation[0]['precision_greater'],\n",
        "      metrics_lr_standard_imputation[1]['precision_greater'],\n",
        "      metrics_lr_standard_imputation[2]['precision_greater'],\n",
        "      metrics_lr_standard_imputation[3]['precision_greater'],\n",
        "      metrics_lr_standard_imputation[4]['precision_greater'],\n",
        "      metrics_lr_original['precision_greater']]  \n",
        "y5_2 = [metrics_svm_standard_imputation[0]['precision_greater'],\n",
        "      metrics_svm_standard_imputation[1]['precision_greater'],\n",
        "      metrics_svm_standard_imputation[2]['precision_greater'],\n",
        "      metrics_svm_standard_imputation[3]['precision_greater'],\n",
        "      metrics_svm_standard_imputation[4]['precision_greater'],\n",
        "      metrics_svm_original['precision_greater']]\n",
        "y6_1 = [metrics_lr_advanced_imputation[0]['precision_greater'],\n",
        "      metrics_lr_advanced_imputation[1]['precision_greater'],\n",
        "      metrics_lr_advanced_imputation[2]['precision_greater'],\n",
        "      metrics_lr_advanced_imputation[3]['precision_greater'],\n",
        "      metrics_lr_advanced_imputation[4]['precision_greater'],\n",
        "      metrics_lr_original['precision_greater']]  \n",
        "y6_2 = [metrics_svm_advanced_imputation[0]['precision_greater'],\n",
        "      metrics_svm_advanced_imputation[1]['precision_greater'],\n",
        "      metrics_svm_advanced_imputation[2]['precision_greater'],\n",
        "      metrics_svm_advanced_imputation[3]['precision_greater'],\n",
        "      metrics_svm_advanced_imputation[4]['precision_greater'],\n",
        "      metrics_svm_original['precision_greater']]\n",
        "\n",
        "# recall of income<=50%\n",
        "y7_1 = [metrics_lr_standard_imputation[0]['recall_smaller'],\n",
        "      metrics_lr_standard_imputation[1]['recall_smaller'],\n",
        "      metrics_lr_standard_imputation[2]['recall_smaller'],\n",
        "      metrics_lr_standard_imputation[3]['recall_smaller'],\n",
        "      metrics_lr_standard_imputation[4]['recall_smaller'],\n",
        "      metrics_lr_original['recall_smaller']]  \n",
        "y7_2 = [metrics_svm_standard_imputation[0]['recall_smaller'],\n",
        "      metrics_svm_standard_imputation[1]['recall_smaller'],\n",
        "      metrics_svm_standard_imputation[2]['recall_smaller'],\n",
        "      metrics_svm_standard_imputation[3]['recall_smaller'],\n",
        "      metrics_svm_standard_imputation[4]['recall_smaller'],\n",
        "      metrics_svm_original['recall_smaller']]\n",
        "y8_1 = [metrics_lr_advanced_imputation[0]['recall_smaller'],\n",
        "      metrics_lr_advanced_imputation[1]['recall_smaller'],\n",
        "      metrics_lr_advanced_imputation[2]['recall_smaller'],\n",
        "      metrics_lr_advanced_imputation[3]['recall_smaller'],\n",
        "      metrics_lr_advanced_imputation[4]['recall_smaller'],\n",
        "      metrics_lr_original['recall_smaller']]  \n",
        "y8_2 = [metrics_svm_advanced_imputation[0]['recall_smaller'],\n",
        "      metrics_svm_advanced_imputation[1]['recall_smaller'],\n",
        "      metrics_svm_advanced_imputation[2]['recall_smaller'],\n",
        "      metrics_svm_advanced_imputation[3]['recall_smaller'],\n",
        "      metrics_svm_advanced_imputation[4]['recall_smaller'],\n",
        "      metrics_svm_original['recall_smaller']]\n",
        "\n",
        "# recall of income>50%\n",
        "y9_1 = [metrics_lr_standard_imputation[0]['recall_greater'],\n",
        "      metrics_lr_standard_imputation[1]['recall_greater'],\n",
        "      metrics_lr_standard_imputation[2]['recall_greater'],\n",
        "      metrics_lr_standard_imputation[3]['recall_greater'],\n",
        "      metrics_lr_standard_imputation[4]['recall_greater'],\n",
        "      metrics_lr_original['recall_greater']]  \n",
        "y9_2 = [metrics_svm_standard_imputation[0]['recall_greater'],\n",
        "      metrics_svm_standard_imputation[1]['recall_greater'],\n",
        "      metrics_svm_standard_imputation[2]['recall_greater'],\n",
        "      metrics_svm_standard_imputation[3]['recall_greater'],\n",
        "      metrics_svm_standard_imputation[4]['recall_greater'],\n",
        "      metrics_svm_original['recall_greater']]\n",
        "y10_1 = [metrics_lr_advanced_imputation[0]['recall_greater'],\n",
        "      metrics_lr_advanced_imputation[1]['recall_greater'],\n",
        "      metrics_lr_advanced_imputation[2]['recall_greater'],\n",
        "      metrics_lr_advanced_imputation[3]['recall_greater'],\n",
        "      metrics_lr_advanced_imputation[4]['recall_greater'],\n",
        "      metrics_lr_original['recall_greater']]  \n",
        "y10_2 = [metrics_svm_advanced_imputation[0]['recall_greater'],\n",
        "      metrics_svm_advanced_imputation[1]['recall_greater'],\n",
        "      metrics_svm_advanced_imputation[2]['recall_greater'],\n",
        "      metrics_svm_advanced_imputation[3]['recall_greater'],\n",
        "      metrics_svm_advanced_imputation[4]['recall_greater'],\n",
        "      metrics_svm_original['recall_greater']]\n",
        "\n",
        "# f1 of income<=50%\n",
        "y11_1 = [metrics_lr_standard_imputation[0]['f1_smaller'],\n",
        "      metrics_lr_standard_imputation[1]['f1_smaller'],\n",
        "      metrics_lr_standard_imputation[2]['f1_smaller'],\n",
        "      metrics_lr_standard_imputation[3]['f1_smaller'],\n",
        "      metrics_lr_standard_imputation[4]['f1_smaller'],\n",
        "      metrics_lr_original['f1_smaller']]  \n",
        "y11_2 = [metrics_svm_standard_imputation[0]['f1_smaller'],\n",
        "      metrics_svm_standard_imputation[1]['f1_smaller'],\n",
        "      metrics_svm_standard_imputation[2]['f1_smaller'],\n",
        "      metrics_svm_standard_imputation[3]['f1_smaller'],\n",
        "      metrics_svm_standard_imputation[4]['f1_smaller'],\n",
        "      metrics_svm_original['f1_smaller']]\n",
        "y12_1 = [metrics_lr_advanced_imputation[0]['f1_smaller'],\n",
        "      metrics_lr_advanced_imputation[1]['f1_smaller'],\n",
        "      metrics_lr_advanced_imputation[2]['f1_smaller'],\n",
        "      metrics_lr_advanced_imputation[3]['f1_smaller'],\n",
        "      metrics_lr_advanced_imputation[4]['f1_smaller'],\n",
        "      metrics_lr_original['f1_smaller']]  \n",
        "y12_2 = [metrics_svm_advanced_imputation[0]['f1_smaller'],\n",
        "      metrics_svm_advanced_imputation[1]['f1_smaller'],\n",
        "      metrics_svm_advanced_imputation[2]['f1_smaller'],\n",
        "      metrics_svm_advanced_imputation[3]['f1_smaller'],\n",
        "      metrics_svm_advanced_imputation[4]['f1_smaller'],\n",
        "      metrics_svm_original['f1_smaller']]\n",
        "\n",
        "# f1 of income>50%\n",
        "y13_1 = [metrics_lr_standard_imputation[0]['f1_greater'],\n",
        "      metrics_lr_standard_imputation[1]['f1_greater'],\n",
        "      metrics_lr_standard_imputation[2]['f1_greater'],\n",
        "      metrics_lr_standard_imputation[3]['f1_greater'],\n",
        "      metrics_lr_standard_imputation[4]['f1_greater'],\n",
        "      metrics_lr_original['f1_greater']]  \n",
        "y13_2 = [metrics_svm_standard_imputation[0]['f1_greater'],\n",
        "      metrics_svm_standard_imputation[1]['f1_greater'],\n",
        "      metrics_svm_standard_imputation[2]['f1_greater'],\n",
        "      metrics_svm_standard_imputation[3]['f1_greater'],\n",
        "      metrics_svm_standard_imputation[4]['f1_greater'],\n",
        "      metrics_svm_original['f1_greater']]\n",
        "y14_1 = [metrics_lr_advanced_imputation[0]['f1_greater'],\n",
        "      metrics_lr_advanced_imputation[1]['f1_greater'],\n",
        "      metrics_lr_advanced_imputation[2]['f1_greater'],\n",
        "      metrics_lr_advanced_imputation[3]['f1_greater'],\n",
        "      metrics_lr_advanced_imputation[4]['f1_greater'],\n",
        "      metrics_lr_original['f1_greater']]  \n",
        "y14_2 = [metrics_svm_advanced_imputation[0]['f1_greater'],\n",
        "      metrics_svm_advanced_imputation[1]['f1_greater'],\n",
        "      metrics_svm_advanced_imputation[2]['f1_greater'],\n",
        "      metrics_svm_advanced_imputation[3]['f1_greater'],\n",
        "      metrics_svm_advanced_imputation[4]['f1_greater'],\n",
        "      metrics_svm_original['f1_greater']]\n",
        "\n",
        "\n",
        "acc_std.set_title('Accuracy standard imputation')\n",
        "acc_std.set_xlabel('Missing values imputed')\n",
        "acc_std.set_ylabel('Accuracy')\n",
        "acc_std.plot(x, y1_1)\n",
        "acc_std.plot(x, y1_2)\n",
        "acc_std.legend(loc='lower right', labels=[\"Log regression\",\"SVM\"])\n",
        "\n",
        "acc_adv.set_title('Accuracy advanced imputation')\n",
        "acc_adv.set_xlabel('Missing values imputed')\n",
        "acc_adv.set_ylabel('Accuracy')\n",
        "acc_adv.plot(x, y2_1)\n",
        "acc_adv.plot(x, y2_2)\n",
        "acc_adv.legend(loc='lower right', labels=[\"Log regression\",\"SVM\"])\n",
        "\n",
        "\n",
        "\n",
        "pre1_std.set_title('Precision \"<=50\" standard imputation')\n",
        "pre1_std.set_xlabel('Missing values imputed')\n",
        "pre1_std.set_ylabel('Precision')\n",
        "pre1_std.plot(x, y3_1)\n",
        "pre1_std.plot(x, y3_2)\n",
        "pre1_std.legend(loc='lower right', labels=[\"Log regression\",\"SVM\"])\n",
        "\n",
        "pre1_adv.set_title('Precision \"<=50\" advanced imputation')\n",
        "pre1_adv.set_xlabel('Missing values imputed')\n",
        "pre1_adv.set_ylabel('Precision')\n",
        "pre1_adv.plot(x, y4_1)\n",
        "pre1_adv.plot(x, y4_2)\n",
        "pre1_adv.legend(loc='lower right', labels=[\"Log regression\",\"SVM\"])\n",
        "\n",
        "\n",
        "pre2_std.set_title('Precision \">50\" standard imputation')\n",
        "pre2_std.set_xlabel('Missing values imputed')\n",
        "pre2_std.set_ylabel('Precision')\n",
        "pre2_std.plot(x, y5_1)\n",
        "pre2_std.plot(x, y5_2)\n",
        "pre2_std.legend(loc='lower right', labels=[\"Log regression\",\"SVM\"])\n",
        "\n",
        "pre2_adv.set_title('Precision \">50\" advanced imputation')\n",
        "pre2_adv.set_xlabel('Missing values imputed')\n",
        "pre2_adv.set_ylabel('Precision')\n",
        "pre2_adv.plot(x, y6_1)\n",
        "pre2_adv.plot(x, y6_2)\n",
        "pre2_adv.legend(loc='lower right', labels=[\"Log regression\",\"SVM\"])\n",
        "\n",
        "\n",
        "rec1_std.set_title('Recall \"<=50\" standard imputation')\n",
        "rec1_std.set_xlabel('Missing values imputed')\n",
        "rec1_std.set_ylabel('Recall')\n",
        "rec1_std.plot(x, y7_1)\n",
        "rec1_std.plot(x, y7_2)\n",
        "rec1_std.legend(loc='lower right', labels=[\"Log regression\",\"SVM\"])\n",
        "\n",
        "rec1_adv.set_title('Recall \"<=50\" advanced imputation')\n",
        "rec1_adv.set_xlabel('Missing values imputed')\n",
        "rec1_adv.set_ylabel('Recall')\n",
        "rec1_adv.plot(x, y8_1)\n",
        "rec1_adv.plot(x, y8_2)\n",
        "rec1_adv.legend(loc='lower right', labels=[\"Log regression\",\"SVM\"])\n",
        "\n",
        "\n",
        "rec2_std.set_title('Recall \">50\" standard imputation')\n",
        "rec2_std.set_xlabel('Missing values imputed')\n",
        "rec2_std.set_ylabel('Recall')\n",
        "rec2_std.plot(x, y9_1)\n",
        "rec2_std.plot(x, y9_2)\n",
        "rec2_std.legend(loc='lower right', labels=[\"Log regression\",\"SVM\"])\n",
        "\n",
        "rec2_adv.set_title('Recall \">50\" advanced imputation')\n",
        "rec2_adv.set_xlabel('Missing values imputed')\n",
        "rec2_adv.set_ylabel('Recall')\n",
        "rec2_adv.plot(x, y10_1)\n",
        "rec2_adv.plot(x, y10_2)\n",
        "rec2_adv.legend(loc='lower right', labels=[\"Log regression\",\"SVM\"])\n",
        "\n",
        "\n",
        "f11_std.set_title('F1 \"<=50\" standard imputation')\n",
        "f11_std.set_xlabel('Missing values imputed')\n",
        "f11_std.set_ylabel('F1')\n",
        "f11_std.plot(x, y11_1)\n",
        "f11_std.plot(x, y11_2)\n",
        "f11_std.legend(loc='lower right', labels=[\"Log regression\",\"SVM\"])\n",
        "\n",
        "f11_adv.set_title('F1 \"<=50\" advanced imputation')\n",
        "f11_adv.set_xlabel('Missing values imputed')\n",
        "f11_adv.set_ylabel('F1')\n",
        "f11_adv.plot(x, y12_1)\n",
        "f11_adv.plot(x, y12_2)\n",
        "f11_adv.legend(loc='lower right', labels=[\"Log regression\",\"SVM\"])\n",
        "\n",
        "\n",
        "f12_std.set_title('F1 \">50\" standard imputation')\n",
        "f12_std.set_xlabel('Missing values imputed')\n",
        "f12_std.set_ylabel('F1')\n",
        "f12_std.plot(x, y13_1)\n",
        "f12_std.plot(x, y13_2)\n",
        "f12_std.legend(loc='lower right', labels=[\"Log regression\",\"SVM\"])\n",
        "\n",
        "f12_adv.set_title('F1 \">50\" advanced imputation')\n",
        "f12_adv.set_xlabel('Missing values imputed')\n",
        "f12_adv.set_ylabel('F1')\n",
        "f12_adv.plot(x, y14_1)\n",
        "f12_adv.plot(x, y14_2)\n",
        "f12_adv.legend(loc='lower right', labels=[\"Log regression\",\"SVM\"])\n",
        "  \n",
        "\n",
        "# Packing all the plots and displaying them\n",
        "#plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HeVk_CQk9Yx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "12_d4hob_cDw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}